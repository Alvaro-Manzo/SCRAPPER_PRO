# -*- coding: utf-8 -*-
"""
Scraper Pro
====================
Herramienta avanzada para extracciÃ³n de datos web con interfaz interactiva.

Autor: Alvaro Manzo
VersiÃ³n: 2.0
Licencia: MIT
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import json
import csv
import logging
import sys
import os
from urllib.parse import urlparse, urljoin
from concurrent.futures import ThreadPoolExecutor
import threading

try:
    from fake_useragent import UserAgent
    HAS_FAKE_USERAGENT = True
except ImportError:
    HAS_FAKE_USERAGENT = False

try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False

# ConfiguraciÃ³n de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scraper.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("MegaScraper")

class MegaScraper:
    """
    Clase principal del Web Scraper con funcionalidades avanzadas.
    """
    
    def __init__(self, use_proxies=False, delay=1.0, timeout=15, verify_ssl=True):
        self.session = requests.Session()
        self.session.verify = verify_ssl
        self.delay = delay
        self.use_proxies = use_proxies
        self.timeout = timeout
        self.proxies = []
        self.visited_urls = set()
        self.data = []
        self.lock = threading.Lock()
        
        # User agents predefinidos
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        ]
        
        # Configurar User Agent
        if HAS_FAKE_USERAGENT:
            try:
                self.ua = UserAgent()
                self.default_user_agent = self.ua.random
            except:
                self.default_user_agent = random.choice(self.user_agents)
        else:
            self.default_user_agent = random.choice(self.user_agents)
        
        self.session.headers.update({"User-Agent": self.default_user_agent})
        
        if self.use_proxies:
            self._load_proxies()

    def _load_proxies(self):
        """Carga proxies desde fuentes pÃºblicas."""
        try:
            sources = [
                'https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt',
                'https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt'
            ]
            
            for source in sources:
                try:
                    response = requests.get(source, timeout=5)
                    if response.status_code == 200:
                        found = [line.strip() for line in response.text.split('\n') 
                               if line.strip() and ':' in line][:50]  # Limitar a 50 proxies
                        self.proxies.extend(found)
                except:
                    continue
                    
            self.proxies = list(set(self.proxies))
            logger.info("[+] {} proxies cargados exitosamente".format(len(self.proxies)))
            
        except Exception as e:
            logger.error("[-] Error cargando proxies: {}".format(e))

    def _get_random_proxy(self):
        """Obtiene un proxy aleatorio."""
        if not self.proxies:
            return None
        proxy = random.choice(self.proxies)
        return {'http': 'http://' + proxy, 'https': 'http://' + proxy}

    def _rotate_headers(self):
        """Rota headers para evitar detecciÃ³n."""
        if HAS_FAKE_USERAGENT and hasattr(self, 'ua'):
            try:
                user_agent = self.ua.random
            except:
                user_agent = random.choice(self.user_agents)
        else:
            user_agent = random.choice(self.user_agents)
            
        headers = {
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Referer': 'https://www.google.com/',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
        }
        self.session.headers.update(headers)
        return headers

    def fetch_url(self, url, max_retries=3, method="GET", data=None, params=None, json_data=None):
        """Obtiene contenido de una URL con manejo de errores."""
        retries = 0
        while retries < max_retries:
            try:
                self._rotate_headers()
                proxy = self._get_random_proxy() if self.use_proxies else None

                if method.upper() == "GET":
                    response = self.session.get(url, timeout=self.timeout, proxies=proxy, params=params)
                elif method.upper() == "POST":
                    response = self.session.post(url, timeout=self.timeout, proxies=proxy, data=data, json=json_data)
                else:
                    response = self.session.request(method, url, timeout=self.timeout, proxies=proxy, 
                                                  data=data, json=json_data, params=params)

                response.raise_for_status()
                sleep_time = random.uniform(self.delay * 0.5, self.delay * 1.5)
                time.sleep(sleep_time)
                return response

            except requests.exceptions.RequestException as e:
                retries += 1
                logger.warning("[!] Error accediendo a {}: {}. Reintento {}/{}".format(
                    url, str(e)[:50], retries, max_retries))
                time.sleep(random.uniform(2, 5))

                if self.use_proxies and self.proxies and proxy and 'http' in proxy:
                    try:
                        proxy_addr = proxy['http'].replace('http://', '')
                        if proxy_addr in self.proxies:
                            self.proxies.remove(proxy_addr)
                    except:
                        pass

        logger.error("[-] No se pudo acceder a {} despuÃ©s de {} intentos".format(url, max_retries))
        return None

    def extract_data(self, url, selectors):
        """Extrae datos segÃºn selectores CSS proporcionados."""
        response = self.fetch_url(url)
        if not response:
            return None

        try:
            soup = BeautifulSoup(response.content, 'html.parser')
            data = {'url': url}

            for field, selector in selectors.items():
                try:
                    elements = soup.select(selector)
                    if not elements:
                        data[field] = None
                        continue

                    if len(elements) > 1:
                        data[field] = [el.get_text(strip=True) for el in elements[:10]]  # Limitar a 10
                    else:
                        data[field] = elements[0].get_text(strip=True)
                        
                except Exception as e:
                    logger.error("Error extrayendo {} con selector {}: {}".format(
                        field, selector, str(e)[:30]))
                    data[field] = None

            return data
            
        except Exception as e:
            logger.error("Error procesando {}: {}".format(url, str(e)[:50]))
            return None

    def extract_links(self, url, link_pattern=None):
        """Extrae enlaces de una pÃ¡gina web."""
        response = self.fetch_url(url)
        if not response:
            return []

        try:
            soup = BeautifulSoup(response.content, 'html.parser')
            base_url = urlparse(url).scheme + '://' + urlparse(url).netloc
            links = []

            if link_pattern:
                elements = soup.select(link_pattern)
                for element in elements:
                    href = element.get('href')
                    if href:
                        full_url = urljoin(base_url, href)
                        links.append(full_url)
            else:
                for a_tag in soup.find_all('a', href=True):
                    href = a_tag.get('href')
                    if href:
                        full_url = urljoin(base_url, href)
                        links.append(full_url)

            # Filtrar enlaces internos Ãºnicos
            internal_links = []
            for link in links:
                if (urlparse(link).netloc == urlparse(url).netloc and 
                    link not in self.visited_urls and 
                    link not in internal_links):
                    internal_links.append(link)

            return internal_links[:200]  # Limitar a 200 enlaces
            
        except Exception as e:
            logger.error("Error extrayendo enlaces: {}".format(str(e)[:50]))
            return []

    def crawl_website(self, start_url, selectors, max_pages=10, depth=2, link_pattern=None):
        """Rastrea un sitio web recursivamente."""
        self.data = []
        self.visited_urls = set()
        urls_to_visit = [(start_url, 0)]

        print("[*] Iniciando crawling...")
        pages_processed = 0
        
        while urls_to_visit and pages_processed < max_pages:
            current_url, current_depth = urls_to_visit.pop(0)

            if current_url in self.visited_urls:
                continue

            self.visited_urls.add(current_url)
            pages_processed += 1
            
            print("[{}/{}] Procesando: {}".format(pages_processed, max_pages, current_url[:60] + "..."))
            logger.info("Visitando {} (profundidad {})".format(current_url, current_depth))

            item_data = self.extract_data(current_url, selectors)
            if item_data:
                self.data.append(item_data)

            if current_depth < depth:
                links = self.extract_links(current_url, link_pattern)
                for link in links:
                    if link not in self.visited_urls and len(urls_to_visit) < max_pages * 2:
                        urls_to_visit.append((link, current_depth + 1))

        print("[+] Crawling completado. {} pÃ¡ginas procesadas.".format(len(self.data)))

    def crawl_multiple_urls(self, urls, selectors, max_workers=5):
        """Extrae datos de mÃºltiples URLs en paralelo."""
        self.data = []
        
        def worker(url):
            data = self.extract_data(url, selectors)
            if data:
                with self.lock:
                    self.data.append(data)
                    print("[+] ExtraÃ­do: {}".format(url[:50] + "..."))

        print("[*] Procesando {} URLs en paralelo...".format(len(urls)))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            executor.map(worker, urls)
            
        print("[+] Procesamiento completado. {} elementos extraÃ­dos.".format(len(self.data)))

    def save_to_csv(self, filename):
        """Guarda datos en formato CSV."""
        if not self.data:
            logger.warning("No hay datos para guardar en CSV")
            return

        try:
            if HAS_PANDAS:
                # Usar pandas si estÃ¡ disponible
                import pandas as pd
                df = pd.DataFrame(self.data)
                df.to_csv(filename, index=False, encoding='utf-8')
            else:
                # Usar csv estÃ¡ndar
                with open(filename, 'w', newline='', encoding='utf-8') as f:
                    if self.data:
                        fieldnames = self.data[0].keys()
                        writer = csv.DictWriter(f, fieldnames=fieldnames)
                        writer.writeheader()
                        for row in self.data:
                            # Convertir listas a strings
                            clean_row = {}
                            for k, v in row.items():
                                if isinstance(v, list):
                                    clean_row[k] = '; '.join(str(x) for x in v)
                                else:
                                    clean_row[k] = str(v) if v is not None else ''
                            writer.writerow(clean_row)
                            
            logger.info("[+] Datos guardados en {}".format(filename))
            
        except Exception as e:
            logger.error("[-] Error guardando CSV: {}".format(e))

    def save_to_json(self, filename):
        """Guarda datos en formato JSON."""
        if not self.data:
            logger.warning("No hay datos para guardar en JSON")
            return

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, ensure_ascii=False, indent=4)
            logger.info("[+] Datos guardados en {}".format(filename))
        except Exception as e:
            logger.error("[-] Error guardando JSON: {}".format(e))

    def extract_table(self, url, table_selector='table'):
        """Extrae tablas HTML de una pÃ¡gina."""
        response = self.fetch_url(url)
        if not response:
            return None

        try:
            soup = BeautifulSoup(response.content, 'html.parser')
            tables = soup.select(table_selector)

            if not tables:
                logger.warning("No se encontraron tablas con el selector {}".format(table_selector))
                return None

            all_tables = []
            for i, table in enumerate(tables):
                try:
                    if HAS_PANDAS:
                        # Usar pandas para leer tablas
                        df = pd.read_html(str(table))[0]
                        all_tables.append(df)
                    else:
                        # Extraer tabla manualmente
                        rows = []
                        for tr in table.find_all('tr'):
                            row = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]
                            if row:
                                rows.append(row)
                        all_tables.append(rows)
                except Exception as e:
                    logger.error("Error procesando tabla {}: {}".format(i, str(e)[:30]))
                    continue

            return all_tables
            
        except Exception as e:
            logger.error("Error extrayendo tablas: {}".format(str(e)[:50]))
            return None

# ============================================================================
# FUNCIONES DE INTERFAZ DE USUARIO
# ============================================================================

def print_banner():
    """Muestra el banner del programa con informaciÃ³n Ãºtil."""
    print("=" * 80)
    print("           ğŸš€ SCRAPPER ğŸš€")
    print("=" * 80)
    print("ğŸŒ Herramienta avanzada para extracciÃ³n de datos web")
    print("")
    print("ğŸ“‹ Â¿QuÃ© puedes hacer con este scraper?")
    print("   â€¢ Extraer datos de cualquier sitio web")
    print("   â€¢ Obtener precios, tÃ­tulos, descripciones automÃ¡ticamente")
    print("   â€¢ Explorar sitios completos siguiendo enlaces")
    print("   â€¢ Procesar mÃºltiples URLs al mismo tiempo")
    print("   â€¢ Convertir tablas HTML a archivos Excel")
    print("   â€¢ Recopilar listas de enlaces")
    print("")
    print("âœ¨ CaracterÃ­sticas tÃ©cnicas:")
    print("   â€¢ RotaciÃ³n automÃ¡tica de User-Agents (evita bloqueos)")
    print("   â€¢ Soporte para proxies (mayor anonimato)")
    print("   â€¢ Procesamiento paralelo (mÃ¡s velocidad)")
    print("   â€¢ ExportaciÃ³n a CSV y JSON")
    print("=" * 80)

def mostrar_menu():
    """Muestra el menÃº principal con explicaciones detalladas."""
    print("\n" + "=" * 80)
    print("                    ğŸ“‹ MENÃš PRINCIPAL")
    print("=" * 80)
    print("")
    print("Â¿QuÃ© tipo de extracciÃ³n quieres realizar?")
    print("")
    print("1ï¸âƒ£  EXTRACCIÃ“N SIMPLE")
    print("    â”” Ideal para: Una sola pÃ¡gina web")
    print("    â”” Ejemplo: Extraer precio de un producto especÃ­fico")
    print("    â”” Uso: RÃ¡pido y directo")
    print("")
    print("2ï¸âƒ£  EXPLORADOR WEB (CRAWLING)")
    print("    â”” Ideal para: Sitios completos (tiendas, blogs, catÃ¡logos)")
    print("    â”” Ejemplo: Obtener todos los productos de una tienda online")
    print("    â”” Uso: AutomÃ¡tico, sigue enlaces internos")
    print("")
    print("3ï¸âƒ£  EXTRACCIÃ“N MÃšLTIPLE")
    print("    â”” Ideal para: Lista de URLs que ya tienes")
    print("    â”” Ejemplo: Comparar precios de un producto en varias tiendas")
    print("    â”” Uso: Procesa muchas URLs en paralelo")
    print("")
    print("4ï¸âƒ£  EXTRACTOR DE TABLAS")
    print("    â”” Ideal para: Datos estructurados (tablas HTML)")
    print("    â”” Ejemplo: Extraer tabla de resultados, estadÃ­sticas")
    print("    â”” Uso: Convierte automÃ¡ticamente a Excel/CSV")
    print("")
    print("5ï¸âƒ£  RECOLECTOR DE ENLACES")
    print("    â”” Ideal para: Obtener todas las URLs de una pÃ¡gina")
    print("    â”” Ejemplo: Encontrar todos los productos de una categorÃ­a")
    print("    â”” Uso: Lista completa de enlaces para usar despuÃ©s")
    print("")
    print("0ï¸âƒ£  SALIR")
    print("    â”” Terminar el programa")
    print("")
    print("=" * 80)

def get_user_input():
    """Obtiene configuraciÃ³n del usuario con explicaciones paso a paso."""
    print("\n" + "ğŸ”§" * 20 + " CONFIGURACIÃ“N DEL SCRAPER " + "ğŸ”§" * 20)
    print("")
    print("Vamos a configurar tu scraper paso a paso...")
    print("")
    
    # ConfiguraciÃ³n de proxies
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ” CONFIGURACIÃ“N DE PROXIES")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("")
    print("ğŸ’¡ Â¿QuÃ© son los proxies?")
    print("   Los proxies ocultan tu direcciÃ³n IP real, Ãºtil para:")
    print("   âœ“ Evitar ser bloqueado por el sitio web")
    print("   âœ“ Acceder a contenido restringido geogrÃ¡ficamente")
    print("   âœ“ Realizar scraping mÃ¡s sigiloso")
    print("")
    print("âš ï¸  NOTA: Los proxies pueden ser mÃ¡s lentos")
    print("")
    use_proxies = input("Â¿Quieres usar proxies? (s/n) [Recomendado para sitios grandes]: ").lower().startswith('s')
    
    # ConfiguraciÃ³n de delay
    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("â±ï¸  VELOCIDAD DE EXTRACCIÃ“N")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("")
    print("ğŸ’¡ Â¿QuÃ© es el delay?")
    print("   Tiempo de espera entre cada peticiÃ³n al servidor:")
    print("   â€¢ 0.5 segundos = MUY RÃPIDO (riesgo de bloqueo)")
    print("   â€¢ 1.0 segundos = RECOMENDADO (equilibrio perfecto)")
    print("   â€¢ 2.0 segundos = CONSERVADOR (muy seguro pero lento)")
    print("   â€¢ 5.0+ segundos = ULTRA SEGURO (para sitios muy estrictos)")
    print("")
    delay_input = input("Delay entre requests en segundos [1.0]: ").strip()
    delay = float(delay_input) if delay_input else 1.0
    
    # ConfiguraciÃ³n de timeout
    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("â° TIEMPO LÃMITE DE ESPERA")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("")
    print("ğŸ’¡ Â¿QuÃ© es el timeout?")
    print("   Tiempo mÃ¡ximo para esperar respuesta del servidor:")
    print("   â€¢ 10 segundos = Conexiones rÃ¡pidas")
    print("   â€¢ 15 segundos = RECOMENDADO (estÃ¡ndar)")
    print("   â€¢ 30 segundos = Conexiones lentas")
    print("")
    timeout_input = input("Timeout por request en segundos [15]: ").strip()
    timeout = int(timeout_input) if timeout_input else 15

    # Mostrar menÃº de opciones
    mostrar_menu()
    
    choice = input("ğŸ‘‰ Selecciona una opciÃ³n (1-5, 0 para salir): ").strip()

    return {
        'use_proxies': use_proxies,
        'delay': delay,
        'timeout': timeout,
        'choice': choice
    }

def configure_selectors():
    """Configura selectores CSS para extracciÃ³n con tutorial interactivo."""
    print("\n" + "ğŸ¯" * 15 + " CONFIGURACIÃ“N DE SELECTORES CSS " + "ğŸ¯" * 15)
    print("")
    print("ğŸ“š TUTORIAL RÃPIDO DE SELECTORES CSS:")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("")
    print("ğŸ” Â¿QuÃ© son los selectores CSS?")
    print("   Son 'direcciones' que le dicen al scraper EXACTAMENTE quÃ© extraer")
    print("")
    print("ğŸ“ EJEMPLOS COMUNES:")
    print("")
    print("   ğŸ·ï¸  Para TÃTULOS:")
    print("       h1                    â†’ Primer tÃ­tulo de la pÃ¡gina")
    print("       .title                â†’ Elementos con clase 'title'")
    print("       #main-title           â†’ Elemento con ID 'main-title'")
    print("       .product-name         â†’ Nombres de productos")
    print("")
    print("   ğŸ’° Para PRECIOS:")
    print("       .price                â†’ Elementos con clase 'price'")
    print("       .cost                 â†’ Elementos con clase 'cost'")
    print("       span.price-value      â†’ Span con clase 'price-value'")
    print("       .product-price        â†’ Precios de productos")
    print("")
    print("   ğŸ“– Para DESCRIPCIONES:")
    print("       .description          â†’ Descripciones generales")
    print("       p                     â†’ Todos los pÃ¡rrafos")
    print("       .summary              â†’ ResÃºmenes")
    print("       .product-details      â†’ Detalles de productos")
    print("")
    print("   ğŸ”— Para ENLACES:")
    print("       a                     â†’ Todos los enlaces")
    print("       .link                 â†’ Enlaces con clase especÃ­fica")
    print("")
    print("   ğŸ–¼ï¸  Para IMÃGENES:")
    print("       img                   â†’ Todas las imÃ¡genes")
    print("       .product-image        â†’ ImÃ¡genes de productos")
    print("")
    print("ğŸ’¡ CONSEJO PRO:")
    print("   1. Ve al sitio web en tu navegador")
    print("   2. Presiona F12 para abrir DevTools")
    print("   3. Haz clic derecho en el elemento que quieres â†’ 'Inspeccionar'")
    print("   4. Busca class='algo' o id='algo' en el cÃ³digo HTML")
    print("   5. Usa .algo para clases o #algo para IDs")
    print("")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("")
    print("ğŸ¯ Ahora vamos a configurar TUS selectores:")
    print("   (Deja un campo vacÃ­o y presiona Enter para terminar)")
    print("")

    selectors = {}
    ejemplos = [
        ("titulo", "h1, .title, .product-name"),
        ("precio", ".price, .cost, .price-value"),
        ("descripcion", ".description, p, .summary"),
        ("imagen", "img, .product-image"),
        ("enlace", "a")
    ]
    
    contador = 1
    while True:
        if contador <= len(ejemplos):
            sugerencia = ejemplos[contador-1]
            field_name = input("{}. Nombre del campo [Sugerencia: '{}']: ".format(contador, sugerencia[0])).strip()
            if not field_name:
                field_name = sugerencia[0]
        else:
            field_name = input("{}. Nombre del campo (ej: categoria, marca, etc.): ".format(contador)).strip()
            
        if not field_name:
            break

        if contador <= len(ejemplos) and field_name == ejemplos[contador-1][0]:
            print("   ğŸ’¡ Sugerencia para '{}': {}".format(field_name, ejemplos[contador-1][1]))
            
        selector = input("   ğŸ¯ Selector CSS para '{}': ".format(field_name)).strip()
        if selector:
            selectors[field_name] = selector
            print("   âœ… Guardado: {} â†’ {}".format(field_name, selector))
        else:
            print("   âš ï¸  Selector vacÃ­o, saltando...")
            
        contador += 1
        print("")

    if selectors:
        print("ğŸ‰ CONFIGURACIÃ“N COMPLETADA:")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        for i, (campo, selector) in enumerate(selectors.items(), 1):
            print("   {}. {}: {}".format(i, campo, selector))
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    else:
        print("âš ï¸  No se configuraron selectores")

    return selectors

def show_sample_data(data, max_items=3):
    """Muestra datos de ejemplo."""
    if not data:
        return
        
    print("\n>>> MUESTRA DE DATOS EXTRAÃDOS:")
    for i, item in enumerate(data[:max_items], 1):
        print("\n--- Elemento {} ---".format(i))
        for key, value in item.items():
            if isinstance(value, list):
                display_value = value[:2] if len(value) > 2 else value
                if len(value) > 2:
                    display_value.append("... y {} mÃ¡s".format(len(value) - 2))
                print("  {}: {}".format(key, display_value))
            else:
                display_value = str(value)[:100]
                if len(str(value)) > 100:
                    display_value += "..."
                print("  {}: {}".format(key, display_value))
    
    if len(data) > max_items:
        print("\n... y {} elementos mÃ¡s".format(len(data) - max_items))

def save_data(scraper, data_name="datos"):
    """Interfaz para guardar datos."""
    if not scraper.data:
        print("[-] No hay datos para guardar")
        return
        
    print("\n>>> OPCIONES DE GUARDADO")
    save_csv = input("Â¿Guardar en CSV? (s/n): ").lower().startswith('s')
    save_json = input("Â¿Guardar en JSON? (s/n): ").lower().startswith('s')
    
    if save_csv or save_json:
        filename = input("Nombre del archivo (sin extensiÃ³n): ").strip() or data_name
        
        if save_csv:
            scraper.save_to_csv("{}.csv".format(filename))
        if save_json:
            scraper.save_to_json("{}.json".format(filename))

def main():
    """FunciÃ³n principal del programa con interfaz mejorada."""
    print_banner()
    
    print("\n" + "ğŸ‰" * 20)
    print("Â¡Bienvenido al Scraper mÃ¡s potente y fÃ¡cil de usar!")
    print("ğŸ‰" * 20)
    print("")
    print("ğŸ“– ANTES DE EMPEZAR:")
    print("   â€¢ Este programa es 100% legal para uso educativo y personal")
    print("   â€¢ Respeta los robots.txt y tÃ©rminos de servicio de los sitios")
    print("   â€¢ Usa delays apropiados para no sobrecargar servidores")
    print("   â€¢ Algunos sitios requieren registro/login previo")
    print("")
    
    continuar = input("Â¿EstÃ¡s listo para empezar? (s/n): ").lower().startswith('s')
    if not continuar:
        print("ğŸ‘‹ Â¡Hasta la prÃ³xima!")
        return

    try:
        while True:  # Bucle principal para mÃºltiples operaciones
            config = get_user_input()
            
            # OpciÃ³n de salir
            if config['choice'] == '0':
                print("\nğŸ‘‹ Â¡Gracias por usar el Scraper!")
                print("ğŸŒŸ Si te sirviÃ³, compÃ¡rtelo con tus amigos desarrolladores!")
                break

            # Inicializar scraper
            print("\n" + "ğŸš€" * 20 + " INICIANDO SCRAPER " + "ğŸš€" * 20)
            print("âš™ï¸  Configurando scraper...")
            if config['use_proxies']:
                print("ğŸ” Cargando proxies...")
            print("ğŸ”„ Configurando rotaciÃ³n de User-Agents...")
            
            scraper = MegaScraper(
                use_proxies=config['use_proxies'],
                delay=config['delay'],
                timeout=config['timeout']
            )
            print("âœ… Scraper configurado correctamente!")

            # OpciÃ³n 1: Extraer datos de una sola URL
            if config['choice'] == '1':
                print("\n" + "ğŸ¯" * 15 + " EXTRACCIÃ“N SIMPLE " + "ğŸ¯" * 15)
                print("")
                print("ğŸ“‹ Vas a extraer datos de UNA SOLA pÃ¡gina web")
                print("ğŸ’¡ Perfecto para obtener informaciÃ³n especÃ­fica rÃ¡pidamente")
                print("")
                
                url = input("ğŸŒ URL a scrapear: ").strip()
                if not url.startswith(('http://', 'https://')):
                    url = 'https://' + url
                    print("ğŸ”§ URL corregida: {}".format(url))
                    
                if not url:
                    print("âŒ URL no vÃ¡lida")
                    continue
                    
                selectors = configure_selectors()

                if selectors:
                    print("\nğŸ” Extrayendo datos de {}...".format(url))
                    data = scraper.extract_data(url, selectors)
                    
                    if data:
                        scraper.data = [data]
                        print("\nâœ… Â¡ExtracciÃ³n exitosa!")
                        show_sample_data(scraper.data)
                        save_data(scraper, "extraccion_simple")
                    else:
                        print("âŒ No se pudieron extraer datos")
                        print("ğŸ’¡ POSIBLES CAUSAS:")
                        print("   â€¢ Selectores CSS incorrectos")
                        print("   â€¢ La pÃ¡gina requiere JavaScript")
                        print("   â€¢ El sitio bloquea bots")
                        print("   â€¢ Problemas de conexiÃ³n")

            # OpciÃ³n 2: Crawlear sitio web completo
            elif config['choice'] == '2':
                print("\n" + "ğŸ•·ï¸" * 15 + " EXPLORADOR WEB " + "ğŸ•·ï¸" * 15)
                print("")
                print("ğŸ“‹ Vas a explorar un sitio web COMPLETO")
                print("ğŸ¤– El scraper seguirÃ¡ enlaces automÃ¡ticamente")
                print("âš ï¸  CUIDADO: Puede ser muy lento en sitios grandes")
                print("")
                
                start_url = input("ğŸŒ URL inicial para crawling: ").strip()
                if not start_url.startswith(('http://', 'https://')):
                    start_url = 'https://' + start_url
                    print("ğŸ”§ URL corregida: {}".format(start_url))
                    
                if not start_url:
                    print("âŒ URL no vÃ¡lida")
                    continue
                    
                selectors = configure_selectors()
                if not selectors:
                    print("âŒ Se necesitan selectores para el crawling")
                    continue
                
                print("\nâš™ï¸  CONFIGURACIÃ“N AVANZADA:")
                print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                print("ğŸ’¡ PÃ¡ginas: CuÃ¡ntas pÃ¡ginas mÃ¡ximo quieres procesar")
                print("ğŸ’¡ Profundidad: QuÃ© tan 'profundo' en el sitio quieres ir")
                print("   â€¢ Profundidad 1: Solo la pÃ¡gina inicial")
                print("   â€¢ Profundidad 2: PÃ¡gina inicial + enlaces directos")
                print("   â€¢ Profundidad 3: Dos niveles mÃ¡s profundo")
                
                max_pages_input = input("ğŸ“„ MÃ¡ximo nÃºmero de pÃ¡ginas [10]: ").strip()
                max_pages = int(max_pages_input) if max_pages_input else 10
                
                depth_input = input("ğŸ—ï¸  Profundidad mÃ¡xima [2]: ").strip()
                depth = int(depth_input) if depth_input else 2
                
                print("ğŸ”— Selector para enlaces (opcional):")
                print("   â€¢ DÃ©jalo vacÃ­o para seguir TODOS los enlaces")
                print("   â€¢ Usa selectores especÃ­ficos para filtrar")
                print("   â€¢ Ejemplo: a.product-link (solo enlaces de productos)")
                link_pattern = input("ğŸ¯ Selector de enlaces: ").strip() or None

                print("\nğŸ•·ï¸  Iniciando crawling...")
                print("â³ Esto puede tomar varios minutos dependiendo del sitio...")
                scraper.crawl_website(start_url, selectors, max_pages, depth, link_pattern)

                if scraper.data:
                    print("\nğŸ‰ Â¡Crawling completado!")
                    print("ğŸ“Š Se extrajeron datos de {} pÃ¡ginas".format(len(scraper.data)))
                    show_sample_data(scraper.data)
                    save_data(scraper, "crawl_completo")
                else:
                    print("âŒ No se pudieron extraer datos durante el crawling")

            # OpciÃ³n 3: Extraer de mÃºltiples URLs
            elif config['choice'] == '3':
                print("\n" + "ğŸ“‹" * 15 + " EXTRACCIÃ“N MÃšLTIPLE " + "ğŸ“‹" * 15)
                print("")
                print("ğŸ“‹ Vas a procesar MÃšLTIPLES URLs al mismo tiempo")
                print("âš¡ Procesamiento paralelo = MÃS VELOCIDAD")
                print("ğŸ’¡ Perfecto para comparar productos, precios, etc.")
                print("")
                
                print("ğŸ”— ENTRADA DE URLs:")
                print("   â€¢ Ingresa una URL por lÃ­nea")
                print("   â€¢ Presiona Enter vacÃ­o para terminar")
                print("   â€¢ MÃ­nimo: 2 URLs, Recomendado: 5-50 URLs")
                print("")
                
                urls = []
                contador = 1
                while True:
                    url = input("{}. URL: ".format(contador)).strip()
                    if not url:
                        break
                    if not url.startswith(('http://', 'https://')):
                        url = 'https://' + url
                    urls.append(url)
                    contador += 1

                if len(urls) < 2:
                    print("âŒ Se necesitan al menos 2 URLs para extracciÃ³n mÃºltiple")
                    continue

                print("\nğŸ“Š URLs cargadas: {}".format(len(urls)))
                
                selectors = configure_selectors()
                if not selectors:
                    print("âŒ Se necesitan selectores")
                    continue
                
                print("\nâš™ï¸  CONFIGURACIÃ“N DE RENDIMIENTO:")
                print("ğŸ’¡ Hilos paralelos: CuÃ¡ntas URLs procesar simultÃ¡neamente")
                print("   â€¢ 3-5 hilos: CONSERVADOR (sitios lentos)")
                print("   â€¢ 5-10 hilos: RECOMENDADO (equilibrio)")
                print("   â€¢ 10+ hilos: AGRESIVO (solo sitios rÃ¡pidos)")
                    
                max_workers_input = input("ğŸ”¥ NÃºmero de hilos paralelos [5]: ").strip()
                max_workers = int(max_workers_input) if max_workers_input else 5

                print("\nâš¡ Procesando {} URLs con {} hilos...".format(len(urls), max_workers))
                scraper.crawl_multiple_urls(urls, selectors, max_workers)

                if scraper.data:
                    print("\nğŸ‰ Â¡ExtracciÃ³n mÃºltiple completada!")
                    print("ğŸ“Š Datos extraÃ­dos de {} URLs".format(len(scraper.data)))
                    show_sample_data(scraper.data)
                    save_data(scraper, "extraccion_multiple")

            # OpciÃ³n 4: Extraer tablas
            elif config['choice'] == '4':
                print("\n" + "ğŸ“Š" * 15 + " EXTRACTOR DE TABLAS " + "ğŸ“Š" * 15)
                print("")
                print("ğŸ“Š Vas a extraer TABLAS HTML y convertirlas a Excel/CSV")
                print("ğŸ’¡ Perfecto para estadÃ­sticas, resultados, precios, etc.")
                print("ğŸ¯ Busca automÃ¡ticamente todas las tablas en la pÃ¡gina")
                print("")
                
                url = input("ğŸŒ URL con tablas: ").strip()
                if not url.startswith(('http://', 'https://')):
                    url = 'https://' + url
                    print("ğŸ”§ URL corregida: {}".format(url))
                    
                if not url:
                    print("âŒ URL no vÃ¡lida")
                    continue
                
                print("\nğŸ¯ SELECTOR DE TABLAS:")
                print("ğŸ’¡ Normalmente 'table' funciona para todas las tablas")
                print("ğŸ’¡ Usa selectores especÃ­ficos para filtrar:")
                print("   â€¢ table.datos â†’ Solo tablas con clase 'datos'")
                print("   â€¢ #tabla1 â†’ Solo la tabla con ID 'tabla1'")
                print("   â€¢ .price-table â†’ Solo tablas de precios")
                    
                table_selector = input("ğŸ“‹ Selector CSS para tablas [table]: ").strip() or "table"

                print("\nğŸ“Š Extrayendo tablas de {}...".format(url))
                tables = scraper.extract_table(url, table_selector)

                if tables:
                    print("\nğŸ‰ Â¡Se encontraron {} tablas!".format(len(tables)))

                    for i, table in enumerate(tables):
                        print("\n" + "â”" * 50)
                        print("ğŸ“Š TABLA {} de {}".format(i + 1, len(tables)))
                        print("â”" * 50)
                        
                        if HAS_PANDAS and hasattr(table, 'head'):
                            print("ğŸ“ Dimensiones: {} filas x {} columnas".format(len(table), len(table.columns)))
                            print("\nğŸ” VISTA PREVIA:")
                            print(table.head())
                            
                            save_table = input("\nğŸ’¾ Â¿Guardar tabla {} en CSV? (s/n): ".format(i + 1)).lower().startswith('s')
                            if save_table:
                                filename = input("ğŸ“ Nombre del archivo [tabla_{}]: ".format(i + 1)).strip() or "tabla_{}".format(i + 1)
                                table.to_csv("{}.csv".format(filename), index=False, encoding='utf-8')
                                print("âœ… Tabla guardada como {}.csv".format(filename))
                        else:
                            print("ğŸ“ Filas encontradas: {}".format(len(table)))
                            print("\nğŸ” VISTA PREVIA (primeras 5 filas):")
                            for j, row in enumerate(table[:5]):
                                print("  {}: {}".format(j + 1, row))
                            if len(table) > 5:
                                print("  ... y {} filas mÃ¡s".format(len(table) - 5))
                                
                            save_table = input("\nğŸ’¾ Â¿Guardar tabla {} en CSV? (s/n): ".format(i + 1)).lower().startswith('s')
                            if save_table:
                                filename = input("ğŸ“ Nombre del archivo [tabla_{}]: ".format(i + 1)).strip() or "tabla_{}".format(i + 1)
                                with open("{}.csv".format(filename), 'w', newline='', encoding='utf-8') as f:
                                    writer = csv.writer(f)
                                    writer.writerows(table)
                                print("âœ… Tabla guardada como {}.csv".format(filename))
                else:
                    print("âŒ No se encontraron tablas")
                    print("ğŸ’¡ POSIBLES CAUSAS:")
                    print("   â€¢ No hay tablas en la pÃ¡gina")
                    print("   â€¢ Selector CSS incorrecto")
                    print("   â€¢ Las tablas se cargan con JavaScript")

            # OpciÃ³n 5: Extraer enlaces
            elif config['choice'] == '5':
                print("\n" + "ğŸ”—" * 15 + " RECOLECTOR DE ENLACES " + "ğŸ”—" * 15)
                print("")
                print("ğŸ”— Vas a extraer TODOS los enlaces de una pÃ¡gina")
                print("ğŸ’¡ Ãštil para encontrar productos, artÃ­culos, categorÃ­as, etc.")
                print("ğŸ“‹ Genera lista completa para usar en extracciÃ³n mÃºltiple")
                print("")
                
                url = input("ğŸŒ URL para extraer enlaces: ").strip()
                if not url.startswith(('http://', 'https://')):
                    url = 'https://' + url
                    print("ğŸ”§ URL corregida: {}".format(url))
                    
                if not url:
                    print("âŒ URL no vÃ¡lida")
                    continue
                
                print("\nğŸ¯ FILTRO DE ENLACES:")
                print("ğŸ’¡ DÃ©jalo vacÃ­o para extraer TODOS los enlaces")
                print("ğŸ’¡ Usa selectores para filtrar enlaces especÃ­ficos:")
                print("   â€¢ a.product â†’ Solo enlaces de productos")
                print("   â€¢ a[href*='categoria'] â†’ Enlaces que contengan 'categoria'")
                print("   â€¢ .menu a â†’ Solo enlaces del menÃº")
                    
                link_pattern = input("ğŸ”— Selector CSS para enlaces: ").strip() or None

                print("\nğŸ” Extrayendo enlaces de {}...".format(url))
                links = scraper.extract_links(url, link_pattern)

                if links:
                    print("\nğŸ‰ Â¡Se encontraron {} enlaces!".format(len(links)))
                    
                    # Mostrar muestra de enlaces
                    print("\nğŸ” MUESTRA DE ENLACES:")
                    print("â”" * 80)
                    for i, link in enumerate(links[:20], 1):
                        print("  {}. {}".format(i, link))

                    if len(links) > 20:
                        print("  ... y {} enlaces mÃ¡s".format(len(links) - 20))

                    save_links = input("\nğŸ’¾ Â¿Guardar enlaces en archivo de texto? (s/n): ").lower().startswith('s')
                    if save_links:
                        filename = input("ğŸ“ Nombre del archivo [enlaces]: ").strip() or "enlaces"
                        with open("{}.txt".format(filename), 'w', encoding='utf-8') as f:
                            for link in links:
                                f.write("{}\n".format(link))
                        print("âœ… {} enlaces guardados en {}.txt".format(len(links), filename))
                        print("ğŸ’¡ Puedes usar este archivo para extracciÃ³n mÃºltiple!")
                else:
                    print("âŒ No se encontraron enlaces")
                    print("ğŸ’¡ POSIBLES CAUSAS:")
                    print("   â€¢ Selector CSS muy restrictivo")
                    print("   â€¢ La pÃ¡gina no tiene enlaces")
                    print("   â€¢ Los enlaces se cargan con JavaScript")

            else:
                print("\nâŒ OpciÃ³n no vÃ¡lida")
                continue
            
            # Preguntar si quiere hacer otra operaciÃ³n
            print("\n" + "ğŸ”„" * 20)
            otra_operacion = input("Â¿Quieres realizar otra extracciÃ³n? (s/n): ").lower().startswith('s')
            if not otra_operacion:
                print("\nğŸ‘‹ Â¡Gracias por usar el scrapper!")
                print("â­ Si te gustÃ³, Â¡compÃ¡rtelo con otros desarrolladores!")
                break

    except KeyboardInterrupt:
        print("\nğŸ›‘ OperaciÃ³n cancelada por el usuario")
        print("ğŸ‘‹ Â¡Hasta la prÃ³xima!")
    except Exception as e:
        print("\nğŸ’¥ Error inesperado: {}".format(e))
        print("ğŸ› Si el error persiste, reporta el problema")
        logger.error("Error en main: {}".format(e))

if __name__ == "__main__":
    main()