# ğŸš€ Mega Web Scraper Pro

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Stable-brightgreen.svg)]()

Herramienta avanzada para extracciÃ³n de datos web con interfaz interactiva en espaÃ±ol. DiseÃ±ada para ser potente, fÃ¡cil de usar y completamente personalizable.

## âœ¨ CaracterÃ­sticas

- ğŸ”„ **RotaciÃ³n automÃ¡tica de User-Agents** - Evita detecciÃ³n
- ğŸ›¡ï¸ **Soporte para proxies** - MÃºltiples fuentes de proxies pÃºblicos
- ğŸ¯ **Selectores CSS personalizables** - Extrae exactamente lo que necesitas
- ğŸ•·ï¸ **Crawling recursivo** - Explora sitios web completos
- âš¡ **Procesamiento paralelo** - MÃºltiples URLs simultÃ¡neamente
- ğŸ“Š **MÃºltiples formatos de salida** - CSV, JSON, TXT
- ğŸ”§ **ExtracciÃ³n de tablas HTML** - Convierte tablas a DataFrames
- ï¿½ **Interfaz en espaÃ±ol** - FÃ¡cil de usar para hispanohablantes
- ğŸ“ **Logging detallado** - Seguimiento completo de operaciones

## ï¿½ InstalaciÃ³n RÃ¡pida

```bash
# Clonar el repositorio
git clone https://github.com/tu-usuario/mega-web-scraper.git
cd mega-web-scraper

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar el scraper
python main.py
```

## ï¿½ Requisitos

- Python 3.7 o superior
- ConexiÃ³n a internet
- LibrerÃ­as especificadas en `requirements.txt`

### Dependencias principales:
- `requests` - Peticiones HTTP
- `beautifulsoup4` - Parsing HTML
- `pandas` - Manejo de datos (opcional)
- `fake-useragent` - RotaciÃ³n de User-Agents (opcional)

## ğŸ¯ Uso

### EjecuciÃ³n bÃ¡sica
```bash
python main.py
```

### Opciones disponibles:

#### 1. **Extraer datos de una sola URL**
Perfecto para pÃ¡ginas individuales. Configura selectores CSS personalizados.

```
URL: https://ejemplo.com/producto
Selectores:
  - titulo: h1
  - precio: .price
  - descripcion: .description
```

#### 2. **Crawlear sitio web completo**
Explora automÃ¡ticamente enlaces internos con control de profundidad.

```
URL inicial: https://blog.ejemplo.com
PÃ¡ginas mÃ¡ximas: 50
Profundidad: 3
```

#### 3. **Extraer de mÃºltiples URLs**
Procesa varias URLs en paralelo para mÃ¡xima eficiencia.

```
URLs:
https://tienda.com/producto1
https://tienda.com/producto2
https://tienda.com/producto3
```

#### 4. **Extraer tablas HTML**
Encuentra y convierte tablas HTML a formato CSV.

#### 5. **Extraer enlaces**
ObtÃ©n todos los enlaces internos de una pÃ¡gina.

## ğŸ”§ Ejemplos de Selectores CSS

### E-commerce
```css
/* Nombre del producto */
h1.product-title

/* Precio */
.price, .product-price

/* DescripciÃ³n */
.description, .product-description

/* Rating */
.rating-value, .stars
```

### Noticias
```css
/* Titular */
h1, .headline, .article-title

/* Autor */
.author, .byline

/* Fecha */
.date, .published-date

/* Contenido */
.article-content, .post-content
```

### Redes Sociales
```css
/* Posts */
.post, .tweet

/* Usuario */
.username, .user-handle

/* Texto */
.post-text, .tweet-text
```

## ğŸ“Š Formatos de Salida

### CSV
- Compatible con Excel y Google Sheets
- CodificaciÃ³n UTF-8
- SeparaciÃ³n automÃ¡tica de listas

### JSON
- Estructura jerÃ¡rquica preservada
- FÃ¡cil procesamiento programÃ¡tico
- Formato legible

### TXT
- Para listas de enlaces
- Una URL por lÃ­nea
- CodificaciÃ³n UTF-8

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Proxies
```python
# El scraper carga automÃ¡ticamente proxies de fuentes pÃºblicas
use_proxies = True  # Activar en la configuraciÃ³n
```

### Delays y Timeouts
```python
delay = 2.0      # Segundos entre requests
timeout = 30     # Timeout por request
```

### ParalelizaciÃ³n
```python
max_workers = 5  # Hilos simultÃ¡neos para mÃºltiples URLs
```

## ğŸ›¡ï¸ Uso Ã‰tico y Legal

### âœ… Buenas PrÃ¡cticas
- Revisa `robots.txt` antes de hacer scraping
- Usa delays apropiados (1-3 segundos)
- No sobrecargues los servidores
- Respeta los tÃ©rminos de servicio
- Solo para uso educativo/personal

### âš ï¸ Consideraciones
- Algunos sitios pueden bloquear bots
- El scraping intensivo puede afectar el rendimiento del sitio
- Siempre verifica la legalidad en tu jurisdicciÃ³n

## ğŸ“ Estructura del Proyecto

```
mega-web-scraper/
â”œâ”€â”€ main.py              # Archivo principal
â”œâ”€â”€ requirements.txt     # Dependencias
â”œâ”€â”€ README.md           # Este archivo
â”œâ”€â”€ LICENSE             # Licencia MIT
â”œâ”€â”€ scraper.log         # Logs (generado automÃ¡ticamente)
â””â”€â”€ datos/              # Archivos de salida (generado automÃ¡ticamente)
    â”œâ”€â”€ *.csv
    â”œâ”€â”€ *.json
    â””â”€â”€ *.txt
```

## ğŸ” Ejemplos de Uso

### Scraping de productos
```bash
# Ejecutar scraper
python main.py

# ConfiguraciÃ³n:
# - Proxies: No (para pruebas)
# - Delay: 1.5 segundos
# - OpciÃ³n: 1 (Una URL)
# - URL: https://tienda.com/producto
# - Selectores:
#   * nombre: h1.product-name
#   * precio: .price-current
#   * stock: .stock-status
```

### Crawling de blog
```bash
# ConfiguraciÃ³n:
# - Proxies: SÃ­ (para sitios grandes)
# - Delay: 2.0 segundos
# - OpciÃ³n: 2 (Crawling)
# - URL: https://blog.ejemplo.com
# - PÃ¡ginas: 100
# - Profundidad: 2
# - Selectores:
#   * titulo: h1.entry-title
#   * fecha: .entry-date
#   * contenido: .entry-content
```

## ğŸ› SoluciÃ³n de Problemas

### Error de dependencias
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### Sitio bloquea requests
- Activar proxies en configuraciÃ³n
- Aumentar delay entre requests
- Verificar que el sitio permita scraping

### Selectores no funcionan
- Inspeccionar HTML con herramientas de desarrollo
- Usar selectores mÃ¡s especÃ­ficos
- Verificar que el contenido no se carga dinÃ¡micamente

### Problemas de encoding
- El scraper usa UTF-8 por defecto
- Verificar configuraciÃ³n regional del sistema

## ğŸ“ Logs

Los logs se guardan automÃ¡ticamente en `scraper.log`:
- Timestamp de cada operaciÃ³n
- URLs visitadas
- Errores encontrados
- EstadÃ­sticas de rendimiento

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## â­ CrÃ©ditos

- Desarrollado con â¤ï¸ para la comunidad hispana
- Inspirado en herramientas como Scrapy y BeautifulSoup
- Contribuciones bienvenidas

## ğŸ“ Soporte

Si encuentras bugs o tienes sugerencias:
1. Revisa los logs en `scraper.log`
2. Abre un issue en GitHub
3. Incluye informaciÃ³n detallada del error

---

**Â¡Dale una â­ si este proyecto te fue Ãºtil!**</content>
<parameter name="filePath">/Users/omanzo/VISUALSTUDIOCODE/IDK/README.md
